# Plan 22-02: Workflow Nodes - Fase 2

## Graph Cache + Router Node + End Node

**Data**: 2025-12-24
**Status**: Planning
**Dependência**: Plan 22 Fase 1 (concluída)

---

## Resumo Executivo

A Fase 1 implementou a infraestrutura base de nodes (Manual Trigger + Agent) usando StateGraph. A Fase 2 foca em:

1. **Graph Cache** - Otimização crítica: cachear grafos compilados
2. **Router Node** - Roteamento condicional usando `Command` pattern
3. **End Node** - Nó terminal para clareza visual

**Nota**: "Respond Webhook" foi removido do escopo - não é necessário pois `/invoke` já retorna a resposta diretamente.

---

## 1. Graph Cache (CRÍTICO)

### Problema Atual

```
Cada request:
  HTTP → get_workflow() → build_workflow_graph() → graph.ainvoke()
                              ↑
                         BOTTLENECK
                     (recompila TODA vez)
```

**Custo atual**: 50-200ms por request apenas para compilar o grafo.

### Solução: WorkflowGraphCache

Singleton que cacheia grafos compilados, similar ao `ModelRegistry`.

```python
# src/nodes/graph_cache.py

import asyncio
import hashlib
import json
import logging
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from langgraph.graph.state import CompiledStateGraph
    from langgraph.checkpoint.base import BaseCheckpointSaver
    from langgraph.store.base import BaseStore

logger = logging.getLogger(__name__)


class WorkflowGraphCache:
    """
    Cache singleton para grafos LangGraph compilados.

    Segue o padrão do ModelRegistry:
    - Singleton via __new__
    - Lock async para thread-safety
    - Cache key baseado em workflow_id + hash(flowData)
    """

    _instance: "WorkflowGraphCache | None" = None
    _cache: dict[str, "CompiledStateGraph"]
    _lock: asyncio.Lock
    _checkpointer: "BaseCheckpointSaver | None"
    _store: "BaseStore | None"

    def __new__(cls) -> "WorkflowGraphCache":
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._cache = {}
            cls._instance._lock = asyncio.Lock()
            cls._instance._checkpointer = None
            cls._instance._store = None
        return cls._instance

    def configure(
        self,
        checkpointer: "BaseCheckpointSaver | None" = None,
        store: "BaseStore | None" = None,
    ) -> None:
        """Configura checkpointer e store (chamado no lifespan)."""
        self._checkpointer = checkpointer
        self._store = store

    async def get_or_build(
        self,
        workflow: dict,
    ) -> "CompiledStateGraph":
        """
        Retorna grafo do cache ou compila e cacheia.

        Cache key: workflow_id:hash(flowData)
        """
        cache_key = self._make_cache_key(workflow)

        # Fast path: cache hit (sem lock)
        if cache_key in self._cache:
            logger.debug(f"Graph cache HIT: {cache_key}")
            return self._cache[cache_key]

        # Slow path: build graph (com lock)
        async with self._lock:
            # Double-check após lock
            if cache_key in self._cache:
                return self._cache[cache_key]

            logger.info(f"Graph cache MISS: {cache_key} - building...")

            # Import aqui para evitar circular import
            from nodes.executor import build_workflow_graph

            graph = await build_workflow_graph(
                workflow=workflow,
                checkpointer=self._checkpointer,
                store=self._store,
            )

            self._cache[cache_key] = graph
            logger.info(f"Graph cached: {cache_key}")

            return graph

    def invalidate(self, workflow_id: str) -> int:
        """
        Invalida todos os grafos de um workflow.

        Retorna número de entradas removidas.
        """
        prefix = f"{workflow_id}:"
        keys_to_remove = [k for k in self._cache if k.startswith(prefix)]

        for key in keys_to_remove:
            del self._cache[key]

        if keys_to_remove:
            logger.info(f"Cache invalidated: {workflow_id} ({len(keys_to_remove)} entries)")

        return len(keys_to_remove)

    def clear(self) -> int:
        """Limpa todo o cache. Retorna número de entradas removidas."""
        count = len(self._cache)
        self._cache.clear()
        logger.info(f"Cache cleared: {count} entries")
        return count

    def stats(self) -> dict:
        """Retorna estatísticas do cache."""
        return {
            "entries": len(self._cache),
            "workflow_ids": list(set(k.split(":")[0] for k in self._cache)),
        }

    def _make_cache_key(self, workflow: dict) -> str:
        """
        Gera cache key determinística.

        Formato: {workflow_id}:{hash_12chars}
        """
        workflow_id = workflow.get("id", "unknown")
        flow_data = workflow.get("flowData", {})

        # Serializa flowData de forma determinística
        flow_json = json.dumps(flow_data, sort_keys=True, ensure_ascii=True)
        flow_hash = hashlib.md5(flow_json.encode()).hexdigest()[:12]

        return f"{workflow_id}:{flow_hash}"


# Singleton instance
workflow_graph_cache = WorkflowGraphCache()
```

### Integração

#### 1. Lifespan (service.py)

```python
# Após configurar checkpointer e store
from nodes.graph_cache import workflow_graph_cache
workflow_graph_cache.configure(checkpointer=saver, store=store)
```

#### 2. Workflow Router (invoke/stream)

```python
# ANTES
graph = await build_workflow_graph(workflow, checkpointer, store)

# DEPOIS
from nodes.graph_cache import workflow_graph_cache
graph = await workflow_graph_cache.get_or_build(workflow)
```

#### 3. Invalidação (update/delete)

```python
# workflow_router.py - update_workflow_endpoint
updated = await update_workflow(store, workflow_id, updates_dict)
workflow_graph_cache.invalidate(workflow_id)

# workflow_router.py - delete_workflow_endpoint
deleted = await delete_workflow(store, workflow_id)
workflow_graph_cache.invalidate(workflow_id)
```

---

## 2. Router Node

### Conceito

Router node usa o `Command` pattern do LangGraph para:
1. Avaliar uma expressão baseada no state
2. Retornar `Command(goto="node_id")` para rotear

```
          ┌──────────────────┐
          │    router-1      │
          │   expression:    │
          │    "source"      │
          └────────┬─────────┘
                   │
      ┌────────────┼────────────┐
      ▼            ▼            ▼
"manual"       "whatsapp"   (default)
      │            │            │
      ▼            ▼            ▼
┌──────────┐ ┌───────────┐ ┌──────────┐
│ agent-1  │ │   wa-1    │ │ fallback │
└──────────┘ └───────────┘ └──────────┘
```

### Implementação

```python
# src/nodes/logic/router_node.py

from typing import Any, Literal
from langgraph.types import Command
from langchain_core.runnables import RunnableConfig

from nodes.base import BaseNode
from nodes.registry import node_registry


@node_registry.register
class RouterNode(BaseNode):
    """
    Router node - roteamento condicional baseado em expressão.

    Config:
        expression: Campo do state a avaliar (ex: "source", "trigger_data.type")
        outputs: Lista de outputs possíveis [{"key": "value", "target": "node_id"}]
        defaultOutput: Node ID padrão se nenhum match

    Exemplo de config:
        {
            "expression": "source",
            "outputs": [
                {"key": "manual", "target": "agent-chat"},
                {"key": "whatsapp", "target": "agent-wa"}
            ],
            "defaultOutput": "agent-fallback"
        }
    """

    node_type = "router"

    def __init__(self, node_id: str, config: dict[str, Any]):
        super().__init__(node_id, config)

        # Validação
        if not config.get("outputs"):
            raise ValueError(f"Router '{node_id}': 'outputs' é obrigatório")
        if not config.get("defaultOutput"):
            raise ValueError(f"Router '{node_id}': 'defaultOutput' é obrigatório")

    async def execute(
        self,
        state: dict[str, Any],
        config: RunnableConfig,
    ) -> Command:
        """
        Avalia expressão e retorna Command com goto.

        Não modifica o state, apenas roteia.
        """
        expression = self.config.get("expression", "source")
        outputs = self.config.get("outputs", [])
        default_output = self.config["defaultOutput"]

        # Avalia expressão (suporta dot notation)
        value = self._evaluate_expression(expression, state)

        # Encontra output correspondente
        target = default_output
        for output in outputs:
            if output.get("key") == value:
                target = output.get("target", default_output)
                break

        return Command(update={}, goto=target)

    def _evaluate_expression(self, expression: str, state: dict) -> str:
        """
        Avalia expressão no state.

        Suporta dot notation: "trigger_data.sender" → state["trigger_data"]["sender"]
        """
        parts = expression.split(".")
        value = state

        for part in parts:
            if isinstance(value, dict):
                value = value.get(part)
            else:
                return ""

        return str(value) if value is not None else ""
```

### Mudança no Executor

O executor precisa **pular edges** de routers, pois o `Command` cuida do roteamento:

```python
# src/nodes/executor.py - Na função build_workflow_graph

# Add edges
for edge in edges:
    source = edge["source"]
    target = edge["target"]

    # Encontra source node
    source_node = next((n for n in nodes if n["id"] == source), None)

    # Router usa Command(goto=...), não precisa de edges explícitas
    if source_node and source_node["type"] == "router":
        continue

    # ... resto do código existente
```

---

## 3. End Node

### Conceito

End node é um **pass-through** que marca visualmente o fim do workflow. Útil para:
- Clareza no editor visual
- Múltiplos pontos de término
- Futuro: métricas de conclusão

```python
# src/nodes/terminal/end_node.py

import logging
from typing import Any

from langchain_core.runnables import RunnableConfig

from nodes.base import BaseNode
from nodes.registry import node_registry

logger = logging.getLogger(__name__)


@node_registry.register
class EndNode(BaseNode):
    """
    End node - marca término do workflow.

    É um pass-through que não modifica o state.
    LangGraph automaticamente conecta ao END.

    Config:
        label: Label opcional para logging (ex: "Success", "Error")
    """

    node_type = "end"

    async def execute(
        self,
        state: dict[str, Any],
        config: RunnableConfig,
    ) -> dict[str, Any]:
        """Pass-through - retorna state sem modificações."""
        label = self.config.get("label", "completed")
        logger.debug(f"Workflow end: {self.node_id} ({label})")
        return {}
```

---

## 4. Estrutura de Arquivos

```
src/nodes/
├── __init__.py          # (existente)
├── base.py              # (existente)
├── registry.py          # (existente)
├── executor.py          # (modificar)
├── graph_cache.py       # NOVO
├── triggers/
│   ├── __init__.py      # (existente)
│   └── ...
├── actions/
│   ├── __init__.py      # (existente)
│   └── ...
├── logic/               # NOVO
│   ├── __init__.py      # NOVO
│   └── router_node.py   # NOVO
└── terminal/            # NOVO
    ├── __init__.py      # NOVO
    └── end_node.py      # NOVO
```

---

## 5. Schema de Workflow

### Exemplo com Router

```json
{
  "id": "wf_ivy_routed",
  "name": "Ivy com Routing",
  "flowData": {
    "nodes": [
      {
        "id": "trigger-1",
        "type": "manual_trigger",
        "name": "Manual Trigger",
        "config": {}
      },
      {
        "id": "router-1",
        "type": "router",
        "name": "Route by Source",
        "config": {
          "expression": "source",
          "outputs": [
            {"key": "manual", "target": "agent-chat"},
            {"key": "whatsapp", "target": "agent-wa"}
          ],
          "defaultOutput": "agent-chat"
        }
      },
      {
        "id": "agent-chat",
        "type": "agent",
        "name": "Chat Agent",
        "config": {
          "prompt": {"system": "Você é a Ivy no chat..."},
          "llm": {"model": "gemini-3-flash-preview"}
        }
      },
      {
        "id": "agent-wa",
        "type": "agent",
        "name": "WhatsApp Agent",
        "config": {
          "prompt": {"system": "Você é a Ivy no WhatsApp..."},
          "llm": {"model": "gemini-3-flash-preview"}
        }
      },
      {
        "id": "end-1",
        "type": "end",
        "name": "End",
        "config": {"label": "completed"}
      }
    ],
    "edges": [
      {"source": "trigger-1", "target": "router-1"},
      {"source": "agent-chat", "target": "end-1"},
      {"source": "agent-wa", "target": "end-1"}
    ]
  }
}
```

**Nota**: Edges do router são omitidas pois `Command(goto=...)` controla o fluxo.

---

## 6. Implementação TDD - Passo a Passo

### Fase 2.1: Graph Cache

#### RED: Escrever testes

```python
# tests/nodes/test_graph_cache.py

import pytest
from unittest.mock import AsyncMock, patch, MagicMock

@pytest.fixture
def sample_workflow():
    return {
        "id": "wf_test",
        "flowData": {
            "nodes": [
                {"id": "trigger-1", "type": "manual_trigger", "config": {}},
                {"id": "agent-1", "type": "agent", "config": {...}},
            ],
            "edges": [{"source": "trigger-1", "target": "agent-1"}],
        },
    }

class TestWorkflowGraphCache:

    def test_singleton_pattern(self):
        """Cache deve ser singleton."""
        from nodes.graph_cache import WorkflowGraphCache
        cache1 = WorkflowGraphCache()
        cache2 = WorkflowGraphCache()
        assert cache1 is cache2

    def test_cache_key_deterministic(self, sample_workflow):
        """Cache key deve ser determinística."""
        from nodes.graph_cache import workflow_graph_cache
        key1 = workflow_graph_cache._make_cache_key(sample_workflow)
        key2 = workflow_graph_cache._make_cache_key(sample_workflow)
        assert key1 == key2

    def test_cache_key_changes_with_flowdata(self, sample_workflow):
        """Cache key deve mudar quando flowData muda."""
        from nodes.graph_cache import workflow_graph_cache
        key1 = workflow_graph_cache._make_cache_key(sample_workflow)

        modified = sample_workflow.copy()
        modified["flowData"]["nodes"].append({"id": "new", "type": "end"})
        key2 = workflow_graph_cache._make_cache_key(modified)

        assert key1 != key2

    @pytest.mark.asyncio
    async def test_cache_hit(self, sample_workflow):
        """Segunda chamada deve usar cache."""
        from nodes.graph_cache import workflow_graph_cache

        with patch("nodes.graph_cache.build_workflow_graph") as mock_build:
            mock_graph = MagicMock()
            mock_build.return_value = mock_graph

            # Primeira chamada - cache miss
            graph1 = await workflow_graph_cache.get_or_build(sample_workflow)
            assert mock_build.call_count == 1

            # Segunda chamada - cache hit
            graph2 = await workflow_graph_cache.get_or_build(sample_workflow)
            assert mock_build.call_count == 1  # Não chamou novamente
            assert graph1 is graph2

    def test_invalidate_removes_entries(self, sample_workflow):
        """Invalidate deve remover entradas do workflow."""
        from nodes.graph_cache import workflow_graph_cache

        # Adiciona ao cache manualmente
        key = workflow_graph_cache._make_cache_key(sample_workflow)
        workflow_graph_cache._cache[key] = MagicMock()

        # Invalida
        removed = workflow_graph_cache.invalidate("wf_test")

        assert removed == 1
        assert key not in workflow_graph_cache._cache

    def test_stats(self):
        """Stats deve retornar info do cache."""
        from nodes.graph_cache import workflow_graph_cache
        stats = workflow_graph_cache.stats()
        assert "entries" in stats
        assert "workflow_ids" in stats
```

#### GREEN: Implementar graph_cache.py

(Código já mostrado acima)

#### REFACTOR: Integrar nos endpoints

Atualizar `workflow_router.py` para usar cache.

---

### Fase 2.2: Router Node

#### RED: Escrever testes

```python
# tests/nodes/test_router.py

import pytest
from langchain_core.runnables import RunnableConfig
from langgraph.types import Command

@pytest.fixture
def router_config():
    return {
        "expression": "source",
        "outputs": [
            {"key": "manual", "target": "agent-chat"},
            {"key": "whatsapp", "target": "agent-wa"},
        ],
        "defaultOutput": "agent-fallback",
    }

class TestRouterNode:

    def test_router_node_type(self):
        """Router deve ter node_type correto."""
        from nodes.logic.router_node import RouterNode
        assert RouterNode.node_type == "router"

    def test_router_requires_outputs(self):
        """Router deve exigir outputs."""
        from nodes.logic.router_node import RouterNode
        with pytest.raises(ValueError, match="outputs"):
            RouterNode("r1", {"defaultOutput": "end"})

    def test_router_requires_default_output(self):
        """Router deve exigir defaultOutput."""
        from nodes.logic.router_node import RouterNode
        with pytest.raises(ValueError, match="defaultOutput"):
            RouterNode("r1", {"outputs": [{"key": "a", "target": "b"}]})

    @pytest.mark.asyncio
    async def test_router_routes_to_matching_output(self, router_config):
        """Router deve rotear para output correspondente."""
        from nodes.logic.router_node import RouterNode

        router = RouterNode("router-1", router_config)
        state = {"source": "whatsapp", "messages": []}
        config = RunnableConfig()

        result = await router.execute(state, config)

        assert isinstance(result, Command)
        assert result.goto == "agent-wa"

    @pytest.mark.asyncio
    async def test_router_uses_default_on_no_match(self, router_config):
        """Router deve usar default quando não há match."""
        from nodes.logic.router_node import RouterNode

        router = RouterNode("router-1", router_config)
        state = {"source": "unknown", "messages": []}
        config = RunnableConfig()

        result = await router.execute(state, config)

        assert result.goto == "agent-fallback"

    @pytest.mark.asyncio
    async def test_router_evaluates_dot_notation(self):
        """Router deve avaliar dot notation."""
        from nodes.logic.router_node import RouterNode

        config = {
            "expression": "trigger_data.type",
            "outputs": [{"key": "connection", "target": "conn-handler"}],
            "defaultOutput": "default",
        }

        router = RouterNode("router-1", config)
        state = {"trigger_data": {"type": "connection"}, "messages": []}

        result = await router.execute(state, RunnableConfig())

        assert result.goto == "conn-handler"

    @pytest.mark.asyncio
    async def test_router_does_not_modify_state(self, router_config):
        """Router não deve modificar state."""
        from nodes.logic.router_node import RouterNode

        router = RouterNode("router-1", router_config)
        state = {"source": "manual", "messages": []}

        result = await router.execute(state, RunnableConfig())

        assert result.update == {}
```

#### GREEN: Implementar router_node.py

(Código já mostrado acima)

---

### Fase 2.3: End Node

#### RED: Escrever testes

```python
# tests/nodes/test_end.py

import pytest
from langchain_core.runnables import RunnableConfig

class TestEndNode:

    def test_end_node_type(self):
        """End deve ter node_type correto."""
        from nodes.terminal.end_node import EndNode
        assert EndNode.node_type == "end"

    @pytest.mark.asyncio
    async def test_end_returns_empty_update(self):
        """End deve retornar update vazio."""
        from nodes.terminal.end_node import EndNode

        end = EndNode("end-1", {"label": "success"})
        state = {"messages": [], "agent_response": "Hello"}

        result = await end.execute(state, RunnableConfig())

        assert result == {}

    @pytest.mark.asyncio
    async def test_end_accepts_empty_config(self):
        """End deve funcionar com config vazio."""
        from nodes.terminal.end_node import EndNode

        end = EndNode("end-1", {})
        result = await end.execute({}, RunnableConfig())

        assert result == {}
```

#### GREEN: Implementar end_node.py

(Código já mostrado acima)

---

### Fase 2.4: Integração no Executor

#### RED: Testes de integração

```python
# tests/nodes/test_executor_router.py

import pytest
from unittest.mock import AsyncMock, patch

@pytest.fixture
def workflow_with_router():
    return {
        "id": "wf_routed",
        "flowData": {
            "nodes": [
                {"id": "trigger-1", "type": "manual_trigger", "config": {}},
                {
                    "id": "router-1",
                    "type": "router",
                    "config": {
                        "expression": "source",
                        "outputs": [{"key": "manual", "target": "end-1"}],
                        "defaultOutput": "end-1",
                    },
                },
                {"id": "end-1", "type": "end", "config": {}},
            ],
            "edges": [
                {"source": "trigger-1", "target": "router-1"},
                # Router edges são ignoradas - Command controla
            ],
        },
    }

class TestExecutorWithRouter:

    @pytest.mark.asyncio
    async def test_executor_skips_router_edges(self, workflow_with_router):
        """Executor deve pular edges de router nodes."""
        from nodes.executor import build_workflow_graph

        graph = await build_workflow_graph(workflow_with_router)

        # Graph deve compilar sem erros
        assert graph is not None

    @pytest.mark.asyncio
    async def test_router_execution_flow(self, workflow_with_router):
        """Workflow com router deve executar corretamente."""
        from nodes.executor import build_workflow_graph
        from langchain_core.messages import HumanMessage

        graph = await build_workflow_graph(workflow_with_router)

        result = await graph.ainvoke({
            "messages": [HumanMessage(content="test")]
        })

        assert "source" in result
        assert result["source"] == "manual"
```

#### GREEN: Atualizar executor.py

Modificar para pular edges de routers.

---

## 7. Checklist de Validação

### Cache
- [ ] Singleton funciona corretamente
- [ ] Cache key é determinística
- [ ] Cache hit rate > 90% para requests repetidos
- [ ] Invalidação funciona em update/delete
- [ ] Logs de cache hit/miss visíveis
- [ ] Performance: <1ms para cache hit

### Router
- [ ] Valida config obrigatória (outputs, defaultOutput)
- [ ] Avalia expressão simples (source)
- [ ] Avalia dot notation (trigger_data.type)
- [ ] Roteia para output correto
- [ ] Usa default quando não há match
- [ ] Retorna Command sem modificar state

### End Node
- [ ] Pass-through (retorna {} vazio)
- [ ] Funciona com config vazio
- [ ] Log de conclusão funciona

### Integração
- [ ] Executor compila workflow com router
- [ ] Executor pula edges de router
- [ ] Streaming funciona com cache
- [ ] Tests passam (todos)

---

## 8. Impacto de Performance Esperado

| Métrica | Antes | Depois |
|---------|-------|--------|
| Tempo build graph | 50-200ms | <1ms (hit) |
| Latência total invoke | 250-400ms | 100-200ms |
| Redução | - | ~50-60% |

---

## 9. Ordem de Implementação

1. **graph_cache.py** + testes
2. **logic/router_node.py** + testes
3. **terminal/end_node.py** + testes
4. **Atualizar executor.py** (pular router edges, imports)
5. **Atualizar workflow_router.py** (usar cache, invalidação)
6. **Atualizar service.py** (configurar cache no lifespan)
7. **Rodar todos os testes**
8. **Rebuild Docker + teste manual**
9. **Medir performance**

---

## 10. Próximos Passos (Fase 3+)

Após Fase 2:
- **Fase 3**: Send WhatsApp Node (envio de mensagens)
- **Fase 4**: WA Connection Trigger + integração webhook
- **Fase 5**: WA Message Trigger + fluxo completo

---

## Referências

- [LangGraph Command Pattern](https://langchain-ai.github.io/langgraph/concepts/low_level/#command)
- [LangGraph Caching Best Practices](https://blog.langchain.com/langgraph-release-week-recap/)
- Plan 22 original: `/docs/plans/plan-22/plan-22.md`
- Análise de exploração: Fase 2 Deep Dive (2025-12-24)
