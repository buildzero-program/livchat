# Plan 13: AST Profiling - Timing Logs

## Contexto

O workflow da Ivy apresenta cold start de **15-18s** em produ√ß√£o, enquanto as subsequentes s√£o r√°pidas (~1-2s).

**Dados do Profiling (PROD):**
```
agent_get_workflow:    0.027s
agent_get_model:       0.000s
stream_first_token:   15.116s  ‚Üê PROBLEMA!
agent_model_invoke:    1.124s
stream_total:         15.152s
```

O LLM (gemini-2.5-flash) est√° r√°pido (1.1s). Os ~14s restantes est√£o em algum lugar ANTES do agent executar.

## Root Cause Identificado

O decorator `@entrypoint()` com par√¢metro `previous` faz o LangGraph carregar o **hist√≥rico do checkpoint** do PostgreSQL **ANTES** de executar a fun√ß√£o:

```
astream_events() chamado
    ‚Üì
LangGraph detecta 'previous' na assinatura
    ‚Üì
Chama checkpointer.aget_state(thread_id) ‚Üê 14 SEGUNDOS AQUI!
    ‚Üì
Injeta 'previous' na fun√ß√£o
    ‚Üì
S√≥ ent√£o executa workflow_agent()
```

## Fase 1: Pontos j√° Instrumentados ‚úÖ

| # | Arquivo | Fun√ß√£o | Status |
|---|---------|--------|--------|
| 1 | `memory/postgres.py` | `get_postgres_saver()` | ‚úÖ |
| 2 | `memory/postgres.py` | `get_postgres_store()` | ‚úÖ |
| 3 | `service/workflow_router.py` | `invoke_workflow()` | ‚úÖ |
| 4 | `service/workflow_router.py` | `workflow_stream_generator()` | ‚úÖ |
| 5 | `agents/workflow_agent.py` | `workflow_agent()` | ‚úÖ |
| 6 | `core/llm.py` | `get_model()` | ‚úÖ |

## Fase 2: Novos Pontos Cr√≠ticos a Instrumentar

| # | Arquivo | Local | O que medir |
|---|---------|-------|-------------|
| 7 | `workflow_router.py` | Antes de `astream_events()` | Tempo at√© chamar o LangGraph |
| 8 | `workflow_router.py` | Dentro do loop de eventos | Tempo at√© primeiro evento chegar |
| 9 | `workflow_agent.py` | **Primeira linha** da fun√ß√£o | Confirmar delay √© ANTES da fun√ß√£o |

## Implementa√ß√£o Fase 2

### Passo 1: Timing ANTES do astream_events

```python
# workflow_router.py - workflow_stream_generator()
# ANTES da linha: async for event in agent.astream_events(

start = start_timer()
log_timing("stream_before_astream_events", stream_start)

async for event in agent.astream_events(...):
    if not first_event_logged:
        log_timing("stream_first_event_received", stream_start)
        first_event_logged = True
```

### Passo 2: Timing na PRIMEIRA LINHA do workflow_agent

```python
# workflow_agent.py - workflow_agent()
@entrypoint()
async def workflow_agent(inputs, *, previous, config, store):
    # PRIMEIRA LINHA - confirmar que o delay √© ANTES disso
    log_timing("workflow_agent_ENTERED", start_timer())  # start_timer retorna tempo atual

    agent_start = start_timer()
    # ... resto do c√≥digo
```

### Passo 3: Testar com thread_id NOVO vs EXISTENTE

Se o problema for carregar hist√≥rico grande:
- Thread novo (sem hist√≥rico): Deve ser r√°pido
- Thread existente (com hist√≥rico): Pode ser lento

## Hip√≥teses de Root Cause

### 1. MAIS PROV√ÅVEL: Neon DB Connection Latency
- Neon Serverless pode ter cold start pr√≥prio
- Connection pool "esfria" ap√≥s per√≠odo ocioso
- SSL handshake + auth em cada nova conex√£o

### 2. PROV√ÅVEL: Checkpoint Query Lenta
- Query no `checkpoints` table com `thread_id`
- √çndice faltando ou ineficiente
- Dados grandes no checkpoint

### 3. POSS√çVEL: Pool Exhaustion
- Saver e Store usam pools separados
- Ambos tentando conex√µes simultaneamente
- Min connections = 5 pode n√£o ser suficiente

## Valida√ß√£o

1. Deploy com novos timings: `flyctl deploy`
2. Testar com thread NOVO: `threadId: "new-test-123"`
3. Testar com thread EXISTENTE: `threadId: "profiling-test-cold"`
4. Comparar logs

## Pr√≥ximos Passos (p√≥s-diagn√≥stico)

Dependendo do gargalo confirmado:

### Se for Neon Connection:
- Aumentar `min_connections` no pool
- Implementar connection keep-alive
- Considerar Neon Pro (menos cold starts)

### Se for Checkpoint Query:
- Adicionar √≠ndice em `thread_id`
- Implementar pruning de hist√≥rico antigo
- Limitar tamanho do checkpoint

### Se for Pool Exhaustion:
- Compartilhar pool entre Saver e Store
- Aumentar `max_connections`
- Implementar retry com backoff

## Output Esperado (Fase 2)

```
‚è±Ô∏è [stream_before_astream_events] 0.001s
‚è±Ô∏è [stream_first_event_received] 14.234s  ‚Üê Confirmado: delay no LangGraph
‚è±Ô∏è [workflow_agent_ENTERED] 14.235s       ‚Üê Confirmado: delay ANTES da fun√ß√£o
‚è±Ô∏è [agent_get_workflow] 0.027s
‚è±Ô∏è [agent_get_model] 0.000s
‚è±Ô∏è [agent_model_invoke] 1.124s
‚è±Ô∏è [stream_first_token] 15.360s
‚è±Ô∏è [stream_total] 15.400s
```

---

## üéØ ROOT CAUSE CONFIRMADO (2025-12-16)

### Erro nos Logs de Produ√ß√£o:

```
psycopg_pool.PoolTimeout: couldn't get a connection after 30.00 sec
```

### An√°lise:

O problema √© **Pool Timeout do psycopg** combinado com **Neon DB Cold Start**:

1. Quando a m√°quina Fly **suspende** (`auto_stop_machines = 'suspend'`), as conex√µes do pool morrem
2. Ao acordar, o pool tenta criar novas conex√µes com o Neon
3. **Neon Serverless** tem cold start pr√≥prio de 5-10s
4. O timeout padr√£o do psycopg_pool √© **30 segundos**
5. Se Fly + Neon est√£o ambos "frios", a lat√™ncia acumula e atinge o timeout

### Stack Trace:

```
workflow_router.py:435 ‚Üí get_workflow(store, workflow_id)
    ‚Üì
storage.py:72 ‚Üí store.aget()
    ‚Üì
langgraph/store/postgres/aio.py:167 ‚Üí get_connection()
    ‚Üì
psycopg_pool/pool_async.py:229 ‚Üí PoolTimeout after 30.00 sec
```

---

## Fase 3: Solu√ß√£o - Pool Configuration Fix

### Problema atual em `memory/postgres.py`:

```python
async with AsyncConnectionPool(
    get_postgres_connection_string(),
    min_size=5,
    max_size=20,
    # FALTA: timeout, open, max_waiting
)
```

### Solu√ß√£o Proposta:

```python
async with AsyncConnectionPool(
    get_postgres_connection_string(),
    min_size=5,
    max_size=20,
    timeout=60,           # Aumentar timeout para 60s (Neon cold start)
    open=True,            # Aguardar conex√µes prontas no startup
    max_waiting=10,       # Limitar fila de espera
    kwargs={...},
    check=AsyncConnectionPool.check_connection,
)
```

### Par√¢metros Adicionados:

| Par√¢metro | Valor | Motivo |
|-----------|-------|--------|
| `timeout` | 60 | Dar tempo para Neon cold start |
| `open` | True | Pool aguarda conex√µes no startup |
| `max_waiting` | 10 | Evita queue infinita |

### Alternativa: Connection Warmup

Adicionar warmup expl√≠cito ap√≥s criar o pool:

```python
# Ap√≥s criar o pool, fazer warmup
async with pool.connection() as conn:
    await conn.execute("SELECT 1")  # For√ßa conex√£o real
```

---

## Deploy da Solu√ß√£o

1. Editar `memory/postgres.py` com novos par√¢metros
2. Adicionar settings para os novos par√¢metros
3. Deploy: `flyctl deploy --now`
4. Testar cold start novamente
5. Monitorar logs

---

## Fase 4: Timing Granular (2025-12-16)

### Objetivo
Descobrir EXATAMENTE onde est√£o os ~14s de delay com logs em CADA ponto.

### Pontos a Instrumentar

#### 1. service/service.py - Lifespan
```
[lifespan_init_start] ‚Üí database pools inicializando
[lifespan_db_contexts_entered] ‚Üí pools prontos
[lifespan_saver_setup_done] ‚Üí checkpointer setup
[lifespan_store_setup_done] ‚Üí store setup
[lifespan_agents_start] ‚Üí carregando agents
[lifespan_agents_all_loaded] ‚Üí agents prontos
```

#### 2. memory/postgres.py - Pool Creation
```
[pool_config] ‚Üí configura√ß√£o do pool
[pool_entered] ‚Üí pool context manager entrou
[checkpointer_before_setup] ‚Üí antes do setup
[store_pool_entered] ‚Üí store pool entrou
[store_before_setup] ‚Üí antes do store setup
```

#### 3. workflow_router.py - Stream Request
```
[stream_config_created] ‚Üí config criado
[stream_calling_astream_events] ‚Üí ANTES de chamar LangGraph
[stream_before_astream] ‚Üê j√° existe
[stream_first_event] ‚Üê j√° existe
```

#### 4. workflow_agent.py - Agent Function
```
[workflow_agent_ENTERED] ‚Üê j√° existe
[agent_params] ‚Üí workflow_id, thread_id
[agent_previous] ‚Üí tamanho do hist√≥rico
[agent_calling_get_workflow] ‚Üí antes de buscar workflow
[agent_calling_get_model] ‚Üí antes de buscar modelo
[agent_invoking_model] ‚Üí antes de chamar LLM
```

### An√°lise dos Gaps

| Gap | Se > 5s | Causa Prov√°vel |
|-----|---------|----------------|
| `lifespan_init_start` ‚Üí `lifespan_db_contexts_entered` | Pool criando conex√µes |
| `stream_calling_astream_events` ‚Üí `workflow_agent_ENTERED` | **Checkpoint loading!** |
| `agent_get_workflow` | Store query lento |
| `agent_model_invoke` | LLM request |

### Hip√≥teses Ranqueadas

1. **Pool frio + min_size alto** (70%) - 5 conex√µes no startup = ~10s
2. **Checkpoint loading** (60%) - Query + deserialize de hist√≥rico grande
3. **Neon cold start** (50%) - Database hibernado
4. **Store query lento** (30%) - √çndice faltando

---

## Fase 5: Implementa√ß√£o das Solu√ß√µes (2025-12-16)

### 5.1 Desligar Suspend no Fly.io ‚úÖ

**Arquivo:** `fly.toml`
```toml
auto_stop_machines = 'off'  # Era 'suspend'
```

**Benef√≠cio:** M√°quina n√£o suspende, conex√µes n√£o morrem.

### 5.2 Pool Warmup com `pool.wait()`

**Problema:** `AsyncConnectionPool` √© LAZY - n√£o cria conex√µes reais no startup.

**Solu√ß√£o:** Usar `await pool.wait(timeout=30.0)` para for√ßar cria√ß√£o de conex√µes.

**C√≥digo:** Em `memory/postgres.py`, ap√≥s criar o pool:
```python
async with AsyncConnectionPool(...) as pool:
    # WARMUP - for√ßa conex√µes reais
    await pool.wait(timeout=30.0)
    # ... resto do c√≥digo
```

**Impacto:**
- Startup: +5-10s (absorve cold start do Neon)
- Primeira request: ~100ms (conex√£o j√° pronta!)

### 5.3 Neon Pooler (PgBouncer)

**Descoberta:** J√° usa hostname `-pooler` mas porta errada!
```
Host: ep-...-pooler.sa-east-1.aws.neon.tech
Porta: 5432 ‚ùå (conex√£o direta)
Porta: 6543 ‚úÖ (via PgBouncer)
```

**Mudan√ßa:**
1. `POSTGRES_PORT=6543`
2. Adicionar `&pgbouncer=true` na connection string
3. Reduzir pool local: `min=1, max=5`

**Benef√≠cio:** PgBouncer mant√©m conex√µes quentes permanentemente.
